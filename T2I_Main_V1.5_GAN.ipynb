{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82d0e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a7d95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few images:\n",
      "[[[219 219 219]\n",
      "  [218 218 218]\n",
      "  [218 218 218]\n",
      "  ...\n",
      "  [240 238 241]\n",
      "  [239 238 243]\n",
      "  [239 238 243]]\n",
      "\n",
      " [[220 220 220]\n",
      "  [219 219 219]\n",
      "  [219 219 219]\n",
      "  ...\n",
      "  [240 238 241]\n",
      "  [239 238 243]\n",
      "  [239 238 243]]\n",
      "\n",
      " [[221 219 222]\n",
      "  [221 219 220]\n",
      "  [222 220 221]\n",
      "  ...\n",
      "  [240 240 242]\n",
      "  [239 239 241]\n",
      "  [239 239 241]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[101 111  87]\n",
      "  [ 81  76  73]\n",
      "  [102  76  87]\n",
      "  ...\n",
      "  [101  99  87]\n",
      "  [ 97 103  89]\n",
      "  [ 96  96  84]]\n",
      "\n",
      " [[ 94  91  82]\n",
      "  [100 106  92]\n",
      "  [ 81  74  68]\n",
      "  ...\n",
      "  [107  97  87]\n",
      "  [102 103  89]\n",
      "  [102  95  85]]\n",
      "\n",
      " [[ 69  72  65]\n",
      "  [103  89  86]\n",
      "  [106 102  90]\n",
      "  ...\n",
      "  [103  95  84]\n",
      "  [ 98 100  86]\n",
      "  [100  96  84]]]\n",
      "[[[ 49  65  26]\n",
      "  [ 47  71  39]\n",
      "  [ 56  69  43]\n",
      "  ...\n",
      "  [ 43  36  43]\n",
      "  [141 163 142]\n",
      "  [190 179 196]]\n",
      "\n",
      " [[ 44  66  17]\n",
      "  [ 43  48  42]\n",
      "  [ 73  72  80]\n",
      "  ...\n",
      "  [157 144 172]\n",
      "  [202 216 219]\n",
      "  [238 255 255]]\n",
      "\n",
      " [[ 39  67   9]\n",
      "  [ 80  77 106]\n",
      "  [ 93 126 161]\n",
      "  ...\n",
      "  [204 191 211]\n",
      "  [126 118 131]\n",
      "  [141 174 153]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 41  90 131]\n",
      "  [ 21  86 144]\n",
      "  [ 54  98 143]\n",
      "  ...\n",
      "  [ 43  42  50]\n",
      "  [ 21  22  27]\n",
      "  [ 14  10   7]]\n",
      "\n",
      " [[ 87 120 129]\n",
      "  [ 53 100 128]\n",
      "  [ 80 104 130]\n",
      "  ...\n",
      "  [ 99  97  85]\n",
      "  [ 53  55  44]\n",
      "  [ 26  30  29]]\n",
      "\n",
      " [[159 151 130]\n",
      "  [138 142 127]\n",
      "  [143 142 121]\n",
      "  ...\n",
      "  [ 84  86  83]\n",
      "  [ 93  95  90]\n",
      "  [ 79  75  74]]]\n",
      "[[[ 89  94  87]\n",
      "  [ 90  91  85]\n",
      "  [ 95 105  97]\n",
      "  ...\n",
      "  [ 90  79  73]\n",
      "  [ 93  78  73]\n",
      "  [ 89  82  76]]\n",
      "\n",
      " [[ 82  91  86]\n",
      "  [ 87  93  91]\n",
      "  [ 95  95  95]\n",
      "  ...\n",
      "  [ 76  84  73]\n",
      "  [ 80  88  73]\n",
      "  [ 83  83  71]]\n",
      "\n",
      " [[ 82  93  85]\n",
      "  [ 81  94  87]\n",
      "  [ 98 104 102]\n",
      "  ...\n",
      "  [ 80  83  72]\n",
      "  [ 82  86  69]\n",
      "  [ 77  85  64]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 97 108 102]\n",
      "  [104 110 108]\n",
      "  [104 116 106]\n",
      "  ...\n",
      "  [ 39  38  46]\n",
      "  [ 39  37  42]\n",
      "  [ 39  37  38]]\n",
      "\n",
      " [[112 114 109]\n",
      "  [109 113 112]\n",
      "  [111 110 106]\n",
      "  ...\n",
      "  [ 40  42  41]\n",
      "  [ 39  41  38]\n",
      "  [ 39  41  38]]\n",
      "\n",
      " [[110 104 104]\n",
      "  [ 99 103 104]\n",
      "  [ 98 100  97]\n",
      "  ...\n",
      "  [ 34  42  45]\n",
      "  [ 34  43  42]\n",
      "  [ 36  42  40]]]\n",
      "[[[141 125 100]\n",
      "  [136 125  93]\n",
      "  [136 128  91]\n",
      "  ...\n",
      "  [ 22  22  12]\n",
      "  [ 16  17   3]\n",
      "  [ 33  34  18]]\n",
      "\n",
      " [[147 131  97]\n",
      "  [145 131 104]\n",
      "  [139 127 103]\n",
      "  ...\n",
      "  [ 21  24  17]\n",
      "  [ 22  25  14]\n",
      "  [ 38  45  29]]\n",
      "\n",
      " [[143 133  98]\n",
      "  [137 131  99]\n",
      "  [138 132 100]\n",
      "  ...\n",
      "  [ 21  26  20]\n",
      "  [ 17  23  19]\n",
      "  [ 23  29  27]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[138 140 127]\n",
      "  [145 147 136]\n",
      "  [140 142 131]\n",
      "  ...\n",
      "  [153 158 126]\n",
      "  [151 156 133]\n",
      "  [137 141 124]]\n",
      "\n",
      " [[145 148 127]\n",
      "  [145 148 129]\n",
      "  [148 153 133]\n",
      "  ...\n",
      "  [156 157 141]\n",
      "  [159 161 148]\n",
      "  [141 144 135]]\n",
      "\n",
      " [[141 145 128]\n",
      "  [150 154 137]\n",
      "  [147 151 136]\n",
      "  ...\n",
      "  [150 150 126]\n",
      "  [156 158 136]\n",
      "  [149 152 131]]]\n",
      "[[[71 64 72]\n",
      "  [72 65 73]\n",
      "  [72 65 73]\n",
      "  ...\n",
      "  [27 27 27]\n",
      "  [27 25 26]\n",
      "  [27 25 26]]\n",
      "\n",
      " [[73 66 74]\n",
      "  [74 67 75]\n",
      "  [74 67 75]\n",
      "  ...\n",
      "  [26 26 26]\n",
      "  [27 25 26]\n",
      "  [27 25 26]]\n",
      "\n",
      " [[73 68 75]\n",
      "  [75 68 76]\n",
      "  [75 68 76]\n",
      "  ...\n",
      "  [28 28 30]\n",
      "  [29 27 30]\n",
      "  [28 26 29]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[24 18 20]\n",
      "  [27 23 24]\n",
      "  [27 23 24]\n",
      "  ...\n",
      "  [ 5  5  5]\n",
      "  [ 5  5  5]\n",
      "  [ 5  5  5]]\n",
      "\n",
      " [[23 19 20]\n",
      "  [28 24 25]\n",
      "  [25 23 24]\n",
      "  ...\n",
      "  [ 5  5  5]\n",
      "  [ 5  5  5]\n",
      "  [ 4  4  4]]\n",
      "\n",
      " [[24 22 23]\n",
      "  [25 23 24]\n",
      "  [23 21 22]\n",
      "  ...\n",
      "  [ 5  5  5]\n",
      "  [ 4  4  4]\n",
      "  [ 4  4  4]]]\n",
      "\n",
      "First few captions:\n",
      "       image_name comment_number  \\\n",
      "0  1000092795.jpg              0   \n",
      "1  1000092795.jpg              1   \n",
      "2  1000092795.jpg              2   \n",
      "3  1000092795.jpg              3   \n",
      "4  1000092795.jpg              4   \n",
      "\n",
      "                                             comment  \n",
      "0  two young guys with shaggy hair look at their ...  \n",
      "1  two young white males are outside near many bu...  \n",
      "2     two men in green shirts are standing in a yard  \n",
      "3         a man in a blue shirt standing in a garden  \n",
      "4              two friends enjoy time spent together  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "processed_image_dir = 'flickr30k_images/processed_images'\n",
    "images = []\n",
    "\n",
    "# Load each .npy file in the directory and add it to the images list\n",
    "for file_name in os.listdir(processed_image_dir):\n",
    "    if file_name.endswith('.npy'):\n",
    "        file_path = os.path.join(processed_image_dir, file_name)\n",
    "        image = np.load(file_path)\n",
    "        images.append(image)\n",
    "\n",
    "# Now, images is a list of numpy arrays (each array is an image)\n",
    "\n",
    "print(\"First few images:\")\n",
    "for img in images[:5]:  \n",
    "    print(img)\n",
    "\n",
    "\n",
    "captions_df = pd.read_csv('flickr30k_images/processed_results.csv')\n",
    "print(\"\\nFirst few captions:\")\n",
    "print(captions_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b5b21fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 16384)             1638400   \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 16384)             65536     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 16384)             0         \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_6 (Conv2D  (None, 16, 16, 128)       819200    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 16, 16, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_7 (Conv2D  (None, 32, 32, 64)        204800    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 32, 32, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_8 (Conv2D  (None, 64, 64, 3)         4800      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2733504 (10.43 MB)\n",
      "Trainable params: 2700352 (10.30 MB)\n",
      "Non-trainable params: 33152 (129.50 KB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 64)        4864      \n",
      "                                                                 \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 128)       204928    \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 32768)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 32769     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 242561 (947.50 KB)\n",
      "Trainable params: 242561 (947.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((8, 8, 256)))  # Reshape to a 3D tensor\n",
    "    assert model.output_shape == (None, 8, 8, 256)\n",
    "\n",
    "    # Upsample to 16x16\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 16, 16, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Upsample to 32x32\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 32, 32, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Upsample to 64x64\n",
    "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 64, 64, 3)\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[64, 64, 3]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "# Print model summaries\n",
    "print(generator.summary())\n",
    "print(discriminator.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53608dd2",
   "metadata": {},
   "source": [
    "problem Faced : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "883a5992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss function and optimizer:\n",
      "<function discriminator_loss at 0x2c8e73560>\n",
      "<keras.src.optimizers.legacy.adam.Adam object at 0x2c8e30410>\n",
      "\n",
      "Generator loss function and optimizer:\n",
      "<function generator_loss at 0x2c8e73420>\n",
      "<keras.src.optimizers.legacy.adam.Adam object at 0x2c8e30a50>\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.legacy.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.legacy.Adam(1e-4)\n",
    "\n",
    "\n",
    "# Print the loss functions and optimizers summary\n",
    "print(\"Discriminator loss function and optimizer:\")\n",
    "print(discriminator_loss)\n",
    "print(discriminator_optimizer)\n",
    "\n",
    "print(\"\\nGenerator loss function and optimizer:\")\n",
    "print(generator_loss)\n",
    "print(generator_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f296656",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7561ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            print(\"Batch shape:\", image_batch.shape)  \n",
    "            train_step(image_batch, None) \n",
    "\n",
    "        print(f'Time for epoch {epoch + 1} is {time.time() - start} sec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3340e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reshaped_images = [np.resize(img, (64, 64, 3)) for img in images]\n",
    "\n",
    "\n",
    "reshaped_images = np.array(reshaped_images)\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(reshaped_images).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e495653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, captions):\n",
    "    noise = tf.random.normal([32, 100])  \n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            # captions are not directly used in the model in this version\n",
    "            train_step(image_batch, None)  # Replace with actual caption handling if needed\n",
    "\n",
    "        print(f'Time for epoch {epoch + 1} is {time.time() - start} sec')\n",
    "        # Generate and save images here if needed\n",
    "\n",
    "# 'dataset' is a tf.data.Dataset of images, and 'epochs' is the number of epochs\n",
    "BATCH_SIZE = 32  \n",
    "train(dataset, 10)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f186972e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 1 is 130.62996983528137 sec\n",
      "Time for epoch 2 is 126.28748679161072 sec\n",
      "Time for epoch 3 is 126.46461701393127 sec\n",
      "Time for epoch 4 is 118.8111400604248 sec\n",
      "Time for epoch 5 is 125.28175115585327 sec\n",
      "Time for epoch 6 is 136.0181758403778 sec\n",
      "Time for epoch 7 is 133.0854148864746 sec\n",
      "Time for epoch 8 is 139.8650348186493 sec\n",
      "Time for epoch 9 is 127.9656629562378 sec\n",
      "Time for epoch 10 is 125.56670308113098 sec\n"
     ]
    }
   ],
   "source": [
    "# Train your model \n",
    "train(dataset, 10)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5cb25be",
   "metadata": {},
   "source": [
    "def generate_image_from_text(text_input, generator_model, captions, noise_dim=100):\n",
    "    # Preprocess the text\n",
    "    processed_text = captions(text_input)  # Replace with your actual text preprocessing steps\n",
    "\n",
    "    # Generate noise\n",
    "    noise = tf.random.normal([1, noise_dim])\n",
    "\n",
    "    # Combine text and noise, if necessary\n",
    "    combined_input = [noise, processed_text]  # Adjust based on your model's input\n",
    "\n",
    "    # Generate the image\n",
    "    generated_image = generator_model(combined_input, training=False)\n",
    "\n",
    "    # Post-process and display the image\n",
    "    plt.imshow(generated_image[0, :, :, :] * 0.5 + 0.5)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "text_input = \"cat\"\n",
    "generate_image_from_text(text_input, generator, captions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95744e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
